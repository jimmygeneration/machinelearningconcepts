{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eb780c",
   "metadata": {},
   "source": [
    "# Purpose Statement\n",
    "\n",
    "The purpose of this notebook is to create a brief reference for introductory machine learning. Thus, the lessons posted here will be \n",
    "\n",
    "1. Concise\n",
    "2. Easily traversable; and \n",
    "3. Written in such a way that it is easily digestible. \n",
    "\n",
    "To the best of my ability, I will keep the mathematical notation consistent. \n",
    "\n",
    "### Table of Contents \n",
    "\n",
    "1. Introduction\n",
    "2. Probability Theory \n",
    "3. Decision Theory \n",
    "4. Probability Distributions \n",
    "5. Linear Models for Regression \n",
    "6. Linear Models for Classification \n",
    "7. Sampling Methods\n",
    "8. Neural Networks \n",
    "9. Mixture Models & Expectation Maximization  \n",
    "10. Continuous Latent Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71e4b8",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "\n",
    "### A simple regression problem\n",
    "\n",
    "Given some data $$f(x) = sin(x) + e$$\n",
    "\n",
    "where e is a random, normally distributed value that gives variation to the data. \n",
    "\n",
    "We can approximate this function using a polynomial function \n",
    "\n",
    "$$y(x, w) = w_0 + w_1x + w_2x^2 + ... + w_Mx^M $$\n",
    "\n",
    "### An introduction to a loss function \n",
    "\n",
    "To find the appropriate w (weights), we minimize the loss function. The sum of squared errors is an example of a loss function \n",
    "\n",
    "$$E(w) = \\frac{1}{2}\\sum_{n=1}^{N}{(y(x_n, w) - t_n)^2} $$\n",
    "\n",
    "The interpretation of this function is \"the sum of the absolute distance between the predicted value and the actual value, squared.\"\n",
    "\n",
    "Note that the square results in larger deviation have larger effects on the value of the loss function. \n",
    "\n",
    "### An introduction to minimizing the loss function\n",
    "\n",
    "We can minimize the function via choosing a value of w for which the E(w) is small as possible. \n",
    "A common method of achieving this is gradient descent. The steps for gradient descent are as follows\n",
    "\n",
    "1. Initialization - start with an initial guess for w. This could be random or based on some criteria. \n",
    "2. Calculate the gradient for that specific point. The gradient represents the vector that points in the direction of the steepest ascent. \n",
    "3. Adjust the parameters in the opposite direction of the gradient. The rate of change of the parameters is based on the gradient and the learning rate, which can be changed according to preferences. \n",
    "4. Repeat steps 2 and 3 until convergence. \n",
    "\n",
    "So why does gradient descent work? \n",
    "\n",
    "We know that the minimum of the loss function must be a minimum where the gradient is equal to zero (for convex loss functions). Gradient descent finds the direction of steepest descent and iterates towards it, guaranteeing that we at least converge to a local minimum. \n",
    "\n",
    "Note that while the basic principle behind gradient descent is straightforward, there are many challenges in practice, such as: \n",
    "- Choosing an appropriate learning rate\n",
    "- Avoiding getting stuck in local minima or saddle points \n",
    "- Handling the high computational cost with large data sets\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
